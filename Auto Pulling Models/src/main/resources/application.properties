spring.application.name=SpringAi
spring.docker.compose.lifecycle-management=start-only
spring.threads.virtual.enabled=true
# If running the Ollama Docker Instance separately, then set this property
spring.docker.compose.enabled=false
#Auto-pulling Models
spring.ai.ollama.init.pull-model-strategy=when_missing
spring.ai.ollama.init.timeout=15m
spring.ai.ollama.init.max-retries=3
# The default Ollama Model in Spring Ai is mistral, but it can be changed by setting the below property.
spring.ai.ollama.chat.options.model=llama3.1
# If additional Models are required, then set this property
#spring.ai.ollama.init.chat.additional-models=llama3.2, qwen2.5