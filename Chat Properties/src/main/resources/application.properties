spring.application.name=SpringAi
spring.docker.compose.lifecycle-management=start-only
spring.threads.virtual.enabled=true
# O llama Chat Model Properties
#Enable O llama chat model.
spring.ai.ollama.chat.enabled=true
#The name of the supported model to use.
#The format to return a response in. Currently, the only accepted value is json
spring.ai.ollama.chat.options.format=json
#Controls how long the model will stay loaded into memory following the request
spring.ai.ollama.chat.options.keep_alive=5m
# The default Ollama Model in Spring Ai is mistral, but it can be changed by setting the property. use same model in entrypoint.sh file
#spring.ai.ollama.chat.options.model=llama3.1
# if Running the Ollama Docker Instance separately, then set this property
spring.docker.compose.enabled=false