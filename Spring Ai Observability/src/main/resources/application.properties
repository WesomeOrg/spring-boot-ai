spring.application.name=SpringAi
spring.threads.virtual.enabled=true
spring.docker.compose.lifecycle-management=start-only
info.app.name=${spring.application.name}
# actuator
management.endpoints.web.base-path=/actuator
management.endpoints.web.exposure.include=*
management.endpoints.jmx.exposure.include=*
management.endpoint.health.show-details=always
management.info.env.enabled=true
management.info.java.enabled=true
management.info.os.enabled=true
# jmx
spring.jmx.enabled=true
# tomcat
server.tomcat.mbeanregistry.enabled=true
# ChatClient input Data
spring.ai.chat.client.observations.include-input=true
# Chat Prompt and Completion Data
spring.ai.chat.observations.include-prompt=true
spring.ai.chat.observations.include-completion=true
spring.ai.chat.observations.include-error-logging=true
# Image Prompt Data
spring.ai.vectorstore.observations.include-query-response=true
spring.ai.image.observations.include-prompt=true
# zipkin
spring.zipkin.base-url=http://localhost:9411
spring.sleuth.sampler.probability=1.0
management.tracing.sampling.probability=1
# The default Ollama Model in Spring Ai is mistral, but it can be changed by setting the property. use same model in entrypoint.sh file
#spring.ai.ollama.chat.options.model=llama3.1
# if Running the Ollama Docker Instance separately, then set this property
spring.docker.compose.enabled=false

