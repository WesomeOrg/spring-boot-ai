spring.application.name=SpringAi
spring.docker.compose.lifecycle-management=start-only
spring.threads.virtual.enabled=true
spring.ai.vectorstore.redis.uri=redis://localhost:6379
spring.ai.vectorstore.redis.index=redis-index
spring.ai.vectorstore.redis.prefix=redis-prefix
spring.ai.vectorstore.redis.initialize-schema=true
# The default Ollama Model in Spring Ai is mistral, but it can be changed by setting the below property. make sure to download the same model in entrypoint.sh file
spring.ai.ollama.chat.options.model=codellama
spring.ai.ollama.embedding.model=codellama
# If running the Ollama Docker Instance separately, then set this property
spring.docker.compose.enabled=false
spring.ai.ollama.chat.options.temperature=100
spring.ai.ollama.embedding.options.temperature=100